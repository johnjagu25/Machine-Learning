{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Intervention System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "\n",
    "#load student data\n",
    "student_data = pd.read_csv('dataset/student-data.csv')\n",
    "target = student_data['passed']\n",
    "student_data.drop('passed',1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of students : 395\n",
      "Number of features : 30\n",
      "Number of passed students : 265\n",
      "Number of failed students : 130\n",
      "Students percentage who got graduation : 67.09%\n"
     ]
    }
   ],
   "source": [
    "n_students,n_features = student_data.shape\n",
    "n_pass = len(target[target=='yes'])\n",
    "n_fail = n_students - n_pass\n",
    "graduation = (n_pass/n_students) * 100\n",
    "\n",
    "print(\"Total Number of students : {}\".format(n_students))\n",
    "print(\"Number of features : {}\".format(n_features))\n",
    "print(\"Number of passed students : {}\".format(n_pass))\n",
    "print(\"Number of failed students : {}\".format(n_fail))\n",
    "print(\"Students percentage who got graduation : {:.2f}%\".format(graduation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess features\n",
    "\n",
    "You can see that, there are several non-numerical values. Since our model understands only numbers,we need to convert everything to numbers. Columns like 'higher','internet' which has values of 'yes'/'no' can be converted to 1/0. And other columns like 'sex','school' which has categorical data can be converted to dummies column using pd.getDummies method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(X):\n",
    "    preprocessed_col = pd.DataFrame(index = X.index)\n",
    "    for col,col_data in X.iteritems():\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes','no'],[1,0])\n",
    "        if col_data.dtype == object :\n",
    "            col_data = pd.get_dummies(col_data,prefix = col)\n",
    "        preprocessed_col = preprocessed_col.join(col_data)\n",
    "    return preprocessed_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_features(student_data)\n",
    "y = target.replace(['yes','no'],[1,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the Dataset\n",
    "\n",
    "so far, we have converted binary and categorical columns to numerical columns. Now we have to prepare training and testing set on the given Dataset. since this would be useful to test our model because testing the unknown dataset with label will be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifiers = [\n",
    "  \n",
    "    { 'name' : \"GaussianNB\", 'clf' : GaussianNB() },\n",
    "    { 'name' : \"Decision Tree\", 'clf' : DecisionTreeClassifier(random_state=42) },\n",
    "    { 'name' : \"Logistic Regression\", 'clf' : LogisticRegression(random_state=42,solver='lbfgs',max_iter=1000) },\n",
    "      { 'name' : \"SVM\", 'clf' : SVC(random_state=42,gamma='auto') }\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "\n",
    "model_list = []\n",
    "count = -1\n",
    "for classifier in classifiers:\n",
    "    for train_size in [200,300]:\n",
    "        \n",
    "        count += 1        \n",
    "        X_train,X_test,y_train,y_test = train_test_split(X,y,train_size = train_size,random_state=42, stratify = y)\n",
    "        \n",
    "        # Setting up a model\n",
    "        clf = classifier['clf']\n",
    "        model = {}  \n",
    "        model['Name'] = classifier['name']\n",
    "        \n",
    "        start = time()\n",
    "        clf.fit(X_train,y_train)\n",
    "        end = time()\n",
    "        \n",
    "        model['Time(training)'] = round(end-start,4)\n",
    "            \n",
    "\n",
    "        \n",
    "        # Prediction  \n",
    "       \n",
    "        predictions_train = clf.predict(X_train)\n",
    "        start = time()\n",
    "        predictions_test = clf.predict(X_test)\n",
    "        end = time()\n",
    "        \n",
    "        # Time calculation\n",
    "        model['Time(prediction)'] = round(end-start,4)\n",
    "        \n",
    "        # Accuracy score - comparing the predicted output to the Actual output\n",
    "        score_train = accuracy_score(y_train,predictions_train)\n",
    "        score_test = accuracy_score(y_test,predictions_test)\n",
    "        model['Score(train)'] = score_train\n",
    "        model['Score(test)'] = score_test\n",
    "                \n",
    "        # Sample size\n",
    "        model['Train_size'] = len(X_train)\n",
    "        model['Test_size'] = len(X_test)   \n",
    "        \n",
    "        #F1 Score \n",
    "        model['F1_score(train)'] = f1_score(y_train,predictions_train,pos_label=1)\n",
    "        model['F1_score(test)'] = f1_score(y_test,predictions_test,pos_label=1)\n",
    "        model_list.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['F1_score(test)', 'F1_score(train)', 'Name', 'Score(test)',\n",
       "       'Score(train)', 'Test_size', 'Time(prediction)', 'Time(training)',\n",
       "       'Train_size'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(model_list)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the columns order\n",
    "cols = ['Name','Train_size','Test_size','Time(training)','Time(prediction)','Score(train)','Score(test)','F1_score(train)','F1_score(test)']\n",
    "df = df[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Train_size</th>\n",
       "      <th>Test_size</th>\n",
       "      <th>Time(training)</th>\n",
       "      <th>Time(prediction)</th>\n",
       "      <th>Score(train)</th>\n",
       "      <th>Score(test)</th>\n",
       "      <th>F1_score(train)</th>\n",
       "      <th>F1_score(test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>200</td>\n",
       "      <td>195</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.814545</td>\n",
       "      <td>0.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>300</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.813397</td>\n",
       "      <td>0.776119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Name  Train_size  Test_size  Time(training)  Time(prediction)  \\\n",
       "0  GaussianNB         200        195          0.0071            0.0039   \n",
       "1  GaussianNB         300         95          0.0040            0.0000   \n",
       "\n",
       "   Score(train)  Score(test)  F1_score(train)  F1_score(test)  \n",
       "0         0.745     0.641026         0.814545        0.740741  \n",
       "1         0.740     0.684211         0.813397        0.776119  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Train_size</th>\n",
       "      <th>Test_size</th>\n",
       "      <th>Time(training)</th>\n",
       "      <th>Time(prediction)</th>\n",
       "      <th>Score(train)</th>\n",
       "      <th>Score(test)</th>\n",
       "      <th>F1_score(train)</th>\n",
       "      <th>F1_score(test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>200</td>\n",
       "      <td>195</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.646154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.743494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>300</td>\n",
       "      <td>95</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.610526</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.683761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Train_size  Test_size  Time(training)  Time(prediction)  \\\n",
       "2  Decision Tree         200        195           0.005             0.004   \n",
       "3  Decision Tree         300         95           0.004             0.004   \n",
       "\n",
       "   Score(train)  Score(test)  F1_score(train)  F1_score(test)  \n",
       "2           1.0     0.646154              1.0        0.743494  \n",
       "3           1.0     0.610526              1.0        0.683761  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Train_size</th>\n",
       "      <th>Test_size</th>\n",
       "      <th>Time(training)</th>\n",
       "      <th>Time(prediction)</th>\n",
       "      <th>Score(train)</th>\n",
       "      <th>Score(test)</th>\n",
       "      <th>F1_score(train)</th>\n",
       "      <th>F1_score(test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>200</td>\n",
       "      <td>195</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.844291</td>\n",
       "      <td>0.768683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>300</td>\n",
       "      <td>95</td>\n",
       "      <td>0.1059</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.776667</td>\n",
       "      <td>0.642105</td>\n",
       "      <td>0.845266</td>\n",
       "      <td>0.746269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name  Train_size  Test_size  Time(training)  \\\n",
       "4  Logistic Regression         200        195          0.1508   \n",
       "5  Logistic Regression         300         95          0.1059   \n",
       "\n",
       "   Time(prediction)  Score(train)  Score(test)  F1_score(train)  \\\n",
       "4            0.0018      0.775000     0.666667         0.844291   \n",
       "5            0.0040      0.776667     0.642105         0.845266   \n",
       "\n",
       "   F1_score(test)  \n",
       "4        0.768683  \n",
       "5        0.746269  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Train_size</th>\n",
       "      <th>Test_size</th>\n",
       "      <th>Time(training)</th>\n",
       "      <th>Time(prediction)</th>\n",
       "      <th>Score(train)</th>\n",
       "      <th>Score(test)</th>\n",
       "      <th>F1_score(train)</th>\n",
       "      <th>F1_score(test)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SVM</td>\n",
       "      <td>200</td>\n",
       "      <td>195</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.712821</td>\n",
       "      <td>0.861736</td>\n",
       "      <td>0.821656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVM</td>\n",
       "      <td>300</td>\n",
       "      <td>95</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.793333</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.866379</td>\n",
       "      <td>0.805195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name  Train_size  Test_size  Time(training)  Time(prediction)  Score(train)  \\\n",
       "6  SVM         200        195          0.0091            0.0061      0.785000   \n",
       "7  SVM         300         95          0.0131            0.0040      0.793333   \n",
       "\n",
       "   Score(test)  F1_score(train)  F1_score(test)  \n",
       "6     0.712821         0.861736        0.821656  \n",
       "7     0.684211         0.866379        0.805195  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[6:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the best model\n",
    "\n",
    "Decision tree performs very worst compared to the other model and we can see that, it is overfitted.\n",
    "\n",
    "Naive Bayes performs better than Logistic and equal to SVM. Its computational time is better than the other model and our costs\n",
    "would not increase when we have more data. Since it doesnt have hyperparameter to tune to increase the performance. So this \n",
    "wont help us in increasing the performance.\n",
    "\n",
    "Logistic Regression performs better than Decision tree but slightly worst than SVM.\n",
    "\n",
    "SVM performs better than other model but equal to NaiveBayes. Its computational time would cost us more when we have more data.\n",
    "\n",
    "Between Naive Bayes and SVM, we will go with svm since it has hyperparameter to tune to increase the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=3, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=1, gamma='auto', kernel='poly',\n",
       "    max_iter=-1, probability=False, random_state=42, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C':[0.01,1,3,5],'kernel':['linear','poly','rbf',],'degree':range(1,6)}\n",
    "\n",
    "clf = SVC(random_state=42,gamma = 'auto')\n",
    "\n",
    "# Make an f1 scoring function using 'make_scorer' \n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "grid_obj = GridSearchCV(clf, parameters,cv=5 , scoring=f1_scorer)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# best estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing on the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score : 0.6947368421052632\n"
     ]
    }
   ],
   "source": [
    "predictions_test = clf.predict(X_test) ;\n",
    "print(\"Accuracy score : {}\" .format(accuracy_score(y_test,predictions_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
